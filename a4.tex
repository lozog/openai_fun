\documentclass{article}

\setlength{\parindent}{2em}

\usepackage{fullpage}
\usepackage{url}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage{hyperref}

% hyperlink setup for table of contents
\hypersetup{
    colorlinks=false, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
}

% I'm actually not sure what these are for
\def\Nat{{\rm I\kern-.17em N}}
\def\SFF{\hbox{I\kern-.09em\hbox{I}}}

% set margins for the nested lists
\setdescription[2]{labelindent=0pt,itemindent=-0.75in}
\setdescription[3]{labelindent=0pt,itemindent=-0.75in}
\setdescription[4]{labelindent=0pt,itemindent=-0.75in}

\newcommand\projecttitle{Assignment 4: Markov Decision Processes and Reinforcement
Learning}
\newcommand\myname{Liam Ozog (20515121)}
\newcommand\myclass{CS486 Spring 2017}
\newcommand\mydate{Due July 21, 2017}

\begin{document}

~\vfill
\begin{center}
\Large

\projecttitle

\myname

\myclass

\mydate

\end{center}
\vfill ~\vfill~

\newpage

\section*{Question 1: Value Iteration}
\normalsize

	\subsection*{Printout of code}

	\textit{See a4q1.py}

  \subsection*{Optimal Policies \& Value Functions}
  \noindent
  \textit{Note: States 15 and 16 have no action in the policy because agents in those states have no choice in their
  movement, thus their actions do not matter.} \\

  \noindent
  Optimal Policy (a = 0.9, b = 0.05):
  \begin{verbatim}
     {0: 'right', 1:  'right', 2:  'right',  3: 'down',   4: 'right',  5: 'right',
      6: 'right', 7:  'down',  8:  'down',   9: 'right', 10: 'right', 11: 'down',
     12: 'right', 13: 'right', 14: 'right', 15: 'none',  16: 'none'}
  \end{verbatim}

  \noindent
  Optimal Value Function (a = 0.9, b = 0.05):
  \begin{verbatim}
    [  86.68355476   88.85264322   91.04238159   93.06759719   85.50610338
       87.61167191   93.1860991    95.34698972   83.91904241   23.93408526
       95.46145877   97.65948678   89.31589593   91.74049353   97.65948678
      100.            0.        ]
  \end{verbatim}

  \noindent
  Optimal Policy (a = 0.8, b = 0.1):
  \begin{verbatim}
    {0: 'right', 1:  'right',  2: 'right',  3: 'down',   4: 'up',     5: 'up',
     6: 'right', 7:  'down',   8: 'down',   9: 'right', 10: 'right', 11: 'down',
    12: 'right', 13: 'right', 14: 'right', 15: 'none',  16: 'none'}
  \end{verbatim}

  \noindent
  Optimal Value Function (a = 0.8, b = 0.1):
  \begin{verbatim}
    [  84.49883009   87.07568686   89.6664856    91.76269958   82.53291104
       85.24817868   92.0553383    94.44598015   74.57203626   22.04530335
       94.71862229   97.19993736   83.34219952   86.75342237   97.19993736
      100.            0.        ]
  \end{verbatim}

  \subsection*{Discussion of Policies \& Value Functions}

  When the values of \texttt{a} decreases and \texttt{b} increases, it means the chances that an action chosen by the
  agent will be succesfully
  carried out \textit{increase}. In other words, the risk of moving into a state that the agent didn't choose goes \textit{down}. In such
  cases, the optimal policy guides the agent to being more cautious - in particular, in states \texttt{4} and \texttt{5},
  the policy guides the agent away from the bad state - despite the fact that the rewards of each state are negative,
  which would normally encourage the agent to get to the goal state as quickly as possible.

\newpage

\section*{Question 2: Q-Learning}
\normalsize

	\subsection*{Printout of code}

	\textit{See a4q2.py}

  \subsection*{Optimal Policies \& Value Functions}
  \noindent
  \textit{Note: States 15 and 16 have no action in the policy because agents in those states have no choice in their
  movement, thus their actions do not matter.} \\

  \noindent
  Optimal Policy ($\epsilon$ = 0.05):
  \begin{verbatim}
     [down down right down right right down down down down down down right right right None None]
  \end{verbatim}

  \noindent
  Optimal Value Function ($\epsilon$ = 0.05):
  \begin{verbatim}
    0: [ 10.95988799  80.64091155  20.78386889   8.27697254]
    1: [ 14.05486802  82.94928726  27.79159739  16.12315828]
    2: [ 15.67918504  56.79107057  -1.99        63.12051181]
    3: [ 28.4125914   82.04173132  30.63231061  24.92403837]
    4: [ 61.93473254  54.78715258  79.06487568  87.9555903 ]
    5: [ 67.5470004   15.60039516  82.44493575  91.47479862]
    6: [ 42.03186989  93.9405557   87.30577331  85.15493934]
    7: [ 41.57242659  93.43558956  76.2269945   66.06446094]
    8: [ -2.18767798  76.33917465  -2.06879163  -3.3115909 ]
    9: [-16.1412991   22.97887292 -30.1405885   -2.27685898]
    10: [ 89.4569798   96.00964789  15.08965073  94.18045185]
    11: [ 71.66758448  97.74418605  78.20509757  95.41196727]
    12: [  9.7376908   32.83902997  17.74316853  90.35206168]
    13: [-19.95157191  66.39817639  60.67769366  95.83484733]
    14: [ 93.3407599  95.285      93.1947671  98.       ]
    15: [ 100.  100.  100.  100.]
    16: [ 0.  0.  0.  0.]
  \end{verbatim}

  \noindent
  Optimal Policy ($\epsilon$ = 0.2):
  \begin{verbatim}
    [down down right down right right right down up down down down right right right None None]
  \end{verbatim}

  \noindent
  Optimal Value Function ($\epsilon$ = 0.2):
  \begin{verbatim}
    0: [ 59.17480474  77.42930356  55.18068918  52.8161637 ]
    1: [ 67.84445959  84.01538043  63.44079431  65.61332503]
    2: [ 68.39814758  82.81981554  73.32842905  87.38508139]
    3: [ 84.55396809  92.99301694  77.25627998  83.07252816]
    4: [ 65.24443039  63.7617358   76.81300654  85.00732151]
    5: [ 73.62075293  17.66866259  77.45985827  90.10851052]
    6: [ 77.26010596  91.85759742  85.04654514  93.46847563]
    7: [ 87.43869871  95.90295099  89.48931419  93.45124741]
    8: [ 76.32033673  66.20380654  57.0914745   14.17398341]
    9: [ 12.30252843  23.48193544  -7.80441434  16.62849994]
    10: [ 86.91590707  95.82439516  14.89014154  91.85609086]
    11: [ 93.1477998   97.98835157  92.84412334  95.5700597 ]
    12: [  5.11160494  48.69630343  15.62462067  90.5305827 ]
    13: [ 13.0705884   88.03635236  73.38151798  95.87497758]
    14: [ 91.61178038  94.6725      91.93837731  98.        ]
    15: [ 100.  100.  100.  100.]
    16: [ 0.  0.  0.  0.]
  \end{verbatim}

  \subsection*{Discussion of impact of $\epsilon$ on convergence}

  $\epsilon$ controls the exploration vs. exploitation of the agent. A higher $\epsilon$ value means the agent will take a
  random action with a higher probability, meaning more exploration. \\

  \noindent
  With an $\epsilon$ of 0.05 (i.e. lower). The agent was greedy and more set on getting to the goal state, so the rate
  of convergance was faster. However, it did not explore enough to find more options for avoiding the bad state. \\

  \noindent
  An $\epsilon$ of 0.2 (i.e. higher) made the agent more willing to explore. This caused the rate of convergence to be
  slower. However, this allowed it to find other ways around the bad state. For example, in state 6, the optimal action
  changed from 1 when $\epsilon$=0.05 (which puts the agent more at risk of accidentally ending up in the bad state)
  to 3, which does not have risk.

\newpage

\section*{Question 3: Deep Q Network}
\normalsize

	\subsection*{Printout of code}

	\textit{See a4q3.py}

  \subsection*{Results}

  \begin{figure}[!ht]
	  \centering
	    \includegraphics[width=1\textwidth]{nerntn}
		\label{fig:nerntn}
	\end{figure}

  \begin{figure}[!ht]
	  \centering
	    \includegraphics[width=1\textwidth]{er}
		\label{fig:er}
	\end{figure}

  \begin{figure}[!ht]
	  \centering
	    \includegraphics[width=1\textwidth]{tn}
		\label{fig:tn}
	\end{figure}

  \begin{figure}[!ht]
	  \centering
	    \includegraphics[width=1\textwidth]{ertn}
		\label{fig:ertn}
	\end{figure}

  \subsection*{Discussion of results}

  With neither experience replay nor a target network, the Deep Q Network was able to learn at a somewhat slow rate.
  After around 300 episodes, the variance in the results began to be very high - in some episodes, the TDR would be as
  high as 50 or more, and the next might be as low as 20. However, the average TDR continues to steadily increase, until
  around episode 600, where the performance seems to hit a plateau. The variance is still high here, but the range of the
  TDR stays between 30 and 60, approximately. \\

  \noindent
  After implementing experience replay, there is a noticeable change. The Deep Q Network starts with very poor performance,
  but starts learning around episode 200, and from there steadily improves until around episode 600, where the TDR begins
  to fall again. The performance falls very quickly, but begins to recover again until the end of the 1000 episode run.
  I am not sure what to attribute the cause of the drop in performance to; one guess is that there were a few outliers of
  \textit{very} low score just before the fall, which can be attributed to the epsilon-greediness sometimes causing the
  agent to play a poor game. These could then have been used when training the agent in the experience replay (i.e.
  if the agent used memories of those poor performances to train itself), causing the performance overall to fall. \\

  \noindent
  With a target network, the network seems to learn quite quickly, beginning around epsiode 200, but with a \textit{lot}
  of variance in the performance. After around episode 300, it seems to swing back and forth between several runs in a
  row of the maximum TDR, then dropping down to the lowest possible TDR. \\

  \noindent
  Finally, with both experience replay and a target network, the performance again learns very quickly. However, during
  this time there is a high amount of variance. After about episode 200, the performance goes up and down at a slower pace,
  - with the occasional terrible TDR score, but a much lower variance in TDR. Around episode 700, there is a sudden spike in the performance. At
  this point, the scores are approaching the maximum, again with the occasional bad score (again, due to the epsilon-greediness). \\

  \noindent
  From seeing these four results, it would appear that the target network helps the agent learn quickly and maximize the
  TDR, and the experience replay helps prevent the TDR from jumping around so much (i.e. lowers the variance). \\

  \noindent
  Something I noticed about these results were their seemingly random nature. Sometimes, I would train the network and
  the graph would have clear, discernable patterns, and other times the network would exhibit unexplainable behaviour.

\end{document}
